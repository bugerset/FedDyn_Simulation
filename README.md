
# FedDyn on CIFAR-10 and MNIST with MobileNet (PyTorch)

This repository provides a PyTorch implementation of FedDyn (Federated Learning based on Dynamic Regularization) using MobileNet on CIFAR-10 and MNIST datasets. 
It is designed to handle Non-IID data distributions effectively by introducing dynamic regularization to mitigate client drift.

## Project Overview 
FedDyn optimizes the global objective by dynamically updating a regularizer for each client. This simulation covers:

Data Partitioning: IID and Non-IID (Dirichlet distribution) splits.

State Management: Handling server state **$h$** and client states **$g_kâ€‹ , Î¸_k$** for dynamic regularization.

Optimized Aggregation: Server-side update logic that specifically excludes BatchNorm parameters from FedDyn regularization to maintain stability.

## Recommended Folder Structure

Your `main.py` imports modules like `data.cifar10`, `fl.client`, etc.  
So the easiest way to run without changing code is to organize files like this:
```
â”œâ”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cifar10.py
â”‚	â”œâ”€â”€ mnist.py
â”‚   â””â”€â”€ partition.py
â”œâ”€â”€ fl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ feddyn.py
â”‚   â””â”€â”€ server.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mobilenet.py
â””â”€â”€ utils/
 	â”œâ”€â”€ __init__.py
 	â”œâ”€â”€ device.py
    â”œâ”€â”€ eval.py
    â”œâ”€â”€ parser.py
    â””â”€â”€ seed.py
```

## Requirements

- Python 3.9+ recommended
- PyTorch + torchvision
- numpy

Run with default settings:
```bash
python main.py
```
Example1: Non-IID
```bash
python main.py --partition niid
```
Example2: Non-IID with control dyn-alpha
```bash
python main.py --train fedavg --partition niid --alpha 0.5 --min-size 10
```

## Device Selection

The code supports:
```
	â€¢	--device auto (default): selects CUDA if available, else MPS (Apple Silicon), else CPU
	â€¢	--device cuda
	â€¢	--device mps
	â€¢	--device cpu
```

Example:
```bash
python main.py --device auto
```

## CLI Arguments

Key arguments (from utils/parser.py):
```
	â€¢	Reproducibility / compute
	â€¢	--seed (default: 845)
	â€¢	--device in {auto,cpu,cuda,mps}

	â€¢	Training method
	â€¢	--dyn-alpha (FedDyn alpha, default 0.1)

	â€¢	Dataset
	â€¢   --data-set (default cifar10, choices=[cifar10, mnist])
	â€¢	--data-root (default ./data)
	â€¢	--augment (train-time augmentation)
	â€¢	--normalize / --no-normalize
	â€¢	--test-batch-size (default 128)

	â€¢	Federated learning config
	â€¢	--num-clients (default 10)
	â€¢	--client-frac fraction of clients sampled per round (default 0.25)
	â€¢	--local-epochs (default 1)
	â€¢	--batch-size (default 100)
	â€¢	--lr learning rate (default 1e-2)
	â€¢	--rounds communication rounds (default 10)

	â€¢	Data partitioning
	â€¢	--partition in {iid,niid}
	â€¢	--alpha: Dirichlet concentration parameter controlling Non-IID severity.
		    â”œâ”€â”€ Î± = 0.1 ~ 0.3: highly skewed label distribution (strong Non-IID)
		  	â”œâ”€â”€	Î± = 0.5: moderate Non-IID (default)
		  	â””â”€â”€	Î± = 0.8 ~ 1.0: closer to IID
	â€¢	--min-size minimum samples per client in non-IID (default 10)
	â€¢	--print-labels / --no-print-labels

	â€¢	Learning rate Scheduler (ReduceOnPlateau)
	â€¢	--lr-factor (learning rate * factor, default 0.5)
	â€¢	--lr-patience (default 5)
	â€¢	--min-lr (deafult 1e-6)
	â€¢	--lr-threshold (default 1e-4)
	â€¢	--lr-cooldown (default 0)
```

Notes on Implementation

1. Client-side Update (fl/feddyn.py)
Each client minimizes a modified loss function that incorporates dynamic regularization to prevent drift from the global objective.
<br>
Local Loss Function: $$L_total = L_task(ğ·;b) âˆ’ âŸ¨ğ·_k^(t-1),ğ·âŸ© + 1/2 * Î±â€‹âˆ¥ğ· - ğ·_k^(t-1)â€‹âˆ¥$$.
â€¢ $L_task$â€‹: Standard Cross-Entropy loss on local batch.
â€¢ $âŸ¨ğ·_k^(t-1)â€‹,ğ·âŸ©$: Linear penalty term using the local gradient state.
â€¢ $1/2 * Î± * âˆ¥ğ· - ğ·_k^(t-1)â€‹âˆ¥$: Quadratic proximal term to keep the model close to the previous global state.

Optimizer: Uses SGD with momentum=0.9 and weight_decay=5e-4.

2. Server-side Aggregation (fl/server.py)
The server maintains a global state h and updates the global model using a corrected averaging scheme.

Server State 'h' Update: $h^(t+1)â€‹=h tâ€‹âˆ’Î±( N1â€‹kâˆˆS tâ€‹âˆ‘â€‹(Î¸ kt+1â€‹âˆ’Î¸ tâ€‹))$
The state h accumulates the average drift (Î”Î¸) across all participating clients.

Global Model Update:

For Learnable Parameters (Weights/Bias): Î¸ t+1= âˆ£S tâ€‹âˆ£1â€‹kâˆˆS tâˆ‘Î¸ kt+1â€‹âˆ’ Î±1â€‹h t+1
â€‹
 
Applies the FedDyn correction term to align with the global optimum.

For BatchNorm Buffers:

Î¸ 
t+1
â€‹
 = 
âˆ£S 
t
â€‹
 âˆ£
1
â€‹
  
kâˆˆS 
t
â€‹
 
âˆ‘
â€‹
 Î¸ 
k
t+1

## Expected Output

Each round prints evaluation results like:
```bash
=== Evaluate global model 1 Round ===
[01] acc=XX.XX%, loss=Y.YYYYYY
=====================================
```
